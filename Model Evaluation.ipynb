{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall, F-1 Score and more\n",
    "\n",
    "In this notebook we're going to cover the concepts of `Preicision` and `Recall` and how we can use these as measurements for the overall performance of a our model. These are useful measures past the traditional measure of accuracy.\n",
    "\n",
    "## Precision\n",
    "\n",
    "The precision of our model is seen as the ratio of `True Positive` values over all `Positive` values the model has evaluated.\n",
    "\n",
    "$$ \\frac{T_p}{T_{p} + F_{p}} $$\n",
    "\n",
    "This is not to be confused with the precision from a statistical perspective which is the the inverse variance of a distribution, or $\\frac{1}{\\sigma^2}$. This is a different value and concept, altogether, which allows us to evaluate the performance of our model, over all.\n",
    "\n",
    "Precision asks the question \"What proportion of positive indentifications was _actually_ correct?\"\n",
    "\n",
    "## Recall\n",
    "\n",
    "The recall is another ratio, this time it is the ratio of the number of `True Positives` over the sum of the `True Positives` and `False Negatives`.\n",
    "\n",
    "$$ \\frac{T_{p}}{T_{p} + F_{n}} $$\n",
    "\n",
    "A system with high recall, and low precision, returns many results but the labels that were predicted for given inputs are mostly incorrect compared to the training labels.\n",
    "\n",
    "Recall attempts to answer the question \"What proportion of actual positives was identified correctly?\"\n",
    "\n",
    "\n",
    "## Model Performance Interpretation\n",
    "\n",
    "### High Precision\n",
    "\n",
    "A system which has high precision, is a system that returns very few total results but the results that it does return are accurate to the training labels that are provided with the evaluated examples. High precision typically related to a low `False Positive` rate, overall.\n",
    "\n",
    "\n",
    "### Perfect Precision and Recall\n",
    "\n",
    "A system that has both a value of `1.0` or 100% for both precision and recall is said to be a perfect predictor of the underlying data generating distribution. Though the reality is that there likely exists enough noise within the underlying data generating system taht we'll eventually come up against [Bayes Error Rate]() which will prevent our model from increasing in performance in either capacity.\n",
    "\n",
    "### Initial Investigative Setup\n",
    "\n",
    "Before we dive into what these metrics for measuring the performance of a model look like we need to setup our experiment by downloading our dataset and splitting it into the respect training, validation, and test sets that allow us to measure the overall performance of our model. We will be leveraging many of the native functionality to the scikit-learn library to help ease this burden.\n",
    "\n",
    "### Breast Cancer Evaluation\n",
    "\n",
    "Using the Breast Cancer dataset that is housed within the [UCI Repository](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)) we can perform some quick `GridSearch` over a simple Support Vector Machine (SVM) model to try and find the best performing hyperparameters of two specific kernels for SVM's, specifically a linear kernel and a radial basis function `RBF` kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and split the training dataset \n",
    "# here we're using the Breast Cancer Wisconsin Dataset\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "# load the breast cancer dataset provided with scikit-learn\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "# Convert labels in the dataframe to integer representations\n",
    "# in this case they'll be binary labels\n",
    "# 0 - benign (no cancer)\n",
    "# 1 - malignant (cancer)\n",
    "\n",
    "# break the dataset into data (x) and targets (y)\n",
    "X = breast_cancer['data']\n",
    "y = breast_cancer['target']\n",
    "\n",
    "# splitting the dataset into training, validation, and test sets\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score --> 0.9849246231155779\n",
      "Best Parameters --> {'clf__C': 0.1, 'clf__kernel': 'linear'}\n",
      "Test Accuracy : 0.9590643274853801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ed/anaconda3/envs/notebooks/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# these examples are borrowed from Python Machine Learning by Sebastian Raschka\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipe_svc = Pipeline([('scl', scaler),\n",
    "                     ('clf', SVC(random_state=1))])\n",
    "\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'clf__C': param_range,\n",
    "               'clf__kernel': ['linear']},\n",
    "              {'clf__C': param_range,\n",
    "               'clf__gamma': param_range,\n",
    "               'clf__kernel': ['rbf']}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_svc,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=100,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(f\"Best Score --> {gs.best_score_}\")\n",
    "print(f\"Best Parameters --> {gs.best_params_}\")\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Test Accuracy : {clf.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Model Performance\n",
    "\n",
    "We can monitor the performance of our model as it is training and understand if we're hitting a point at which are model may start to under or over perform, also known as the bias-variance trade off that is made when selecting models. A model is said to be high bias when it starts to underfit our data in that the model doesn't have sufficient complexity to capture the variance in our dataset collected from a data generating distribution. A model is said to have high variance, or overfits our dataset, when the model's complexity is high enough that is it able to completely fit our datasets with 100% accuracy. You might also hear terms such as a high number of degrees of freedom or parameters to describe this phenomenon.\n",
    "\n",
    "As a side note many researchers and practitioners in the field of Machine Learning generally look to understanding why parametric models such as neural networks with parameter counts far exceeding the dimensionality of the input tend to perform well with respect to generalization error. This is an open problem and many people are working diligently to try an explain this phenomenon.\n",
    "\n",
    "Below we will graph the performance of our (relatively) simple SVM grid-search implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c434ffa610d24e20ad2db82e077ecde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 1.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lreg = LogisticRegression(penalty='l2', random_state=0, solver='newton-cg')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "                    ('scl', scaler),\n",
    "                    ('clf', lreg)])\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr,\n",
    "                                                        X=X_train,\n",
    "                                                        y=y_train,\n",
    "                                                        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                        cv=10,\n",
    "                                                        n_jobs=-1)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "#plt.figure(figsize=(30,30))\n",
    "axes.plot(train_sizes, train_mean, color='blue', \n",
    "         marker='o', markersize=5, label='training_accuracy')\n",
    "\n",
    "axes.fill_between(train_sizes, train_mean + train_std, train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "axes.plot(train_sizes, test_mean, color='green',\n",
    "         linestyle=\"--\", marker='s', markersize=5,\n",
    "         label='validation_accuracy')\n",
    "\n",
    "axes.fill_between(train_sizes, test_mean + test_std, test_mean - test_std,\n",
    "                 alpha = 0.15, color = 'green')\n",
    "\n",
    "axes.grid()\n",
    "axes.set_xlabel(\"Number of training examples\")\n",
    "axes.set_ylabel(\"Accuracy\")\n",
    "axes.legend(loc='lower right')\n",
    "axes.set_ylim([0.8, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eda4d1283e94c579c6ddc4e357afea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 1.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forced Overfitting\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = [0.001, 0.01, 0.1, 10.0, 100.0, 1000.0]\n",
    "train_scores, test_scores = validation_curve(\n",
    "                                             estimator=pipe_lr,\n",
    "                                             X=X_train,\n",
    "                                             y=y_train,\n",
    "                                             param_name='clf__C',\n",
    "                                             param_range=param_range,\n",
    "                                             cv=10)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig1, axes1 = plt.subplots()\n",
    "\n",
    "axes1.plot(param_range, train_mean, color='blue',\n",
    "         marker='o', markersize=5, label=\"training_accuracy\")\n",
    "\n",
    "axes1.fill_between(param_range, train_mean + train_std, train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "axes1.plot(param_range, test_mean, color='green',\n",
    "         linestyle='--', marker='s', markersize=5,\n",
    "         label='validation_accuracy')\n",
    "\n",
    "axes1.fill_between(param_range, test_mean + test_std, test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "axes1.arrow(param_range[-1],train_mean[-1], (0.0), (0.1), fc='k', ec='k', head_width=0.05, length_includes_head=True)\n",
    "axes1.grid()\n",
    "axes1.set_xscale('log')\n",
    "axes1.legend(loc='lower right')\n",
    "axes1.set_xlabel(\"Parameter C\")\n",
    "axes1.set_ylabel(\"Accuracy\")\n",
    "axes1.set_ylim([0.8, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the accuracy on the validation dataset had started to decline while the training accuracy was increasing. This is a sign that the model is overfitting the training dataset. The decline in the validation dataset is indicative of this.\n",
    "\n",
    "Once we have a trained model we can now start to investigate how to interpret the `Precision`, `Recall`, and `F-1` score of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cc6d2f26444519b587af3a0a62eef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '2-Class Precision-Recall curve : AP=0.9977039610004896')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from inspect import signature\n",
    "\n",
    "#We'll reuse our best classifier from the GridSearch we used above\n",
    "\n",
    "y_score = clf.decision_function(x_test)\n",
    "\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "step_kwargs = ({'step': 'post'}\n",
    "              if 'step' in signature(plt.fill_between).parameters\n",
    "              else {})\n",
    "\n",
    "pr_fig, pr_plot = plt.subplots()\n",
    "pr_plot.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "\n",
    "pr_plot.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "pr_plot.set_xlabel(\"Recall\")\n",
    "pr_plot.set_ylabel(\"Precision\")\n",
    "pr_plot.set_ylim([0.0, 1.05])\n",
    "pr_plot.set_xlim([0.0, 1.0])\n",
    "pr_plot.set_title(f\"2-Class Precision-Recall curve : AP={average_precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Bengin       0.95      0.94      0.94        63\n",
      "   Malignant       0.96      0.97      0.97       108\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.96      0.95      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "f1 = f1_score(y_test,  y_pred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Bengin\", \"Malignant\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that the `classification_report` method provided by scitkit-learn provides the `Precision`, `Recall`, and `F1-Score` all in a readable format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
